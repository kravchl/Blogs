## Welcome to Leo's Blog

# Week 2


### References
Latour, Bruno. “On actor-network theory. A few clarifications plus more than a few complications”.  Vol. 25 N° 3 et 4, pp.47-64.
Norman, Donald and Stappers, Pieter. “DesignX: Complex Sociotechnical Systems”. Published March 3, 2016.
Payne, Bryan and Edwards, Keith. “A Brief Introduction to Usable Security”.

# Week 3

### References
Dourish, Paul. “Implications for Design”. Published 2006.
IDEO.org. “The Field Guide to Human-Centered Design”. 1st Edition. Published 2015.
Shilton, Katie. “Values Levers: Building Ethics into Design”. Published 2012.

# Week 4

## Multi-Tussles

  The concept of tussle, just another word for conflict resolution really, posed by Clark in this paper revolves around the idea that different actors that utilize the Internet have different needs or uses for the Internet. The paper goes into detail describing the framework from the birth of the Internet and its end-to-end application development to where we are now. The Internet now while seemingly endless in nature, can still be affected to change as actors with different motives of use enter its realm. The multi-tussle idea that expands on Clark's framework points out that there are multiple conflicts of interest between actors within the Internet's development. These conflicts are not limited to technological needs but cover cultural needs and differences. 
  
  I believe the underlying thought being made is how growing technology, in this case the entirety of the Internet, should be done with respect to a multitude of factors. I would agree that making a bit of software without considering the implications it may have is foolish. I would say the paper covers these implications in a very robust way, offering an almost philosophical approach to technology. 
  
###  To be technology or not to be. 

## Degrees of Freedom, Dimensions of Power

 The Internet has come a long way with the fundamental design being decentralization, we now find ourselves battling with big business objectives and a much more centralized design. Companies that provide Internet access have a scary amount of power in their hands. Privately owned, they can impose their power on a whim, provided there are no laws against it - which in the technologic expansion we're in now, are ever-growing. The article describes this potential imbalance of power as companies seek to structure and perfect their systems from unlawful users which is in direct conflict with the original Internet design. 
  
  While it may be obvious that a strictly wild west approach to the Internet would be disastrous, it was in this state of chaos that allowed for innovation and expansion. Powered with curiosity and freedom, software developers were able to work without restrictions in mind. However, malicious users could also impose this freedom to do harm. The difficulty in the direction the Internet must go falls under a struggle of power between freedom and regulations that restrict them. Allowing too much centralization of Internet power stifles the growth of technology, while not enough enables malicious users to do more harm.
  
  In a perfect world, there wouldn't be bad people. Unfortunately, there are more than people would like to consider. Ultimately, with enough power, it only takes one person to break everything if fail safes and protection aren't in place. 
  
  The paper goes on to expand on social influencers and the power in which they can impose on their users. Facebook has immense behavioral analysis tools that they can use to affect what their users do. Through sheer numbers testing, they can put themselves in a position of power that rivals ISPs. They are an example of how a centralization of data can provide power to a single entity that could prove dangerous in the wrong hands.
  
   The power of data is scary as in previous versions of the Internet, anonymity was almost gauranteed. Now, we find government agencies able to tap into data stashed away that could identify us online by just the behavior of our Internet usage. 
  
  The Internet is a scary, complex, and ever-developing place. We must tread lightly as a whole and not allow for big business to gain complete control over it. Their intentions are rooted in a desire to protect their assets and not necessarily to protect the users. A fully decentralized approach is again, foolish, but the basic principle should remain to provide the freedoms of innovation that built the Internet in the first place. 


## The Contingent Internet
 
  This paper has a similar to theme to the one above. The author poses the question at first whether it's possible for the Internet to not have developed. As if to consider an alternate dimension in which the world was not connected via near instantaneous communcation. It would appear that the Internet was inevitable. 
  
  The experience of the Internet is becoming more diverse and more accessible. Increases in usage means increases in risk to its users. Providing universal protection such as encryption to the users is certainly possible, however not entirely implimented as companies still want to track users to provide customized experiences as well as safety reasons to prevent malicious users. Again, we find ourselves with the problem of aloting the power from ISPs to users. Who should get what rights?

### References
Benkler, Yochai. “Degrees of Freedom, Dimensions of Power”. Published 2016.
Clark, David. “The Contingent Internet”. Published 2016.
O’Hara, Dustin. “Internet Tussles – a framework for analyzing heterogeneous networks”. Published 2020.

# Week 5

## Surveillance & Society

  In our growing networked world, the need for privacy is more important than ever, but the boundaries on what and who this privacy applies to isn’t well defined. Submitting DNA into a laboratories data base not only exposed one’s personal data into the world, but their immediate family as well as potential future kids. It’s data such as this that is being used to provide personalized experiences on the web for individuals. The more data our computer systems have on us, the more related content we will receive. 
  
  The problem in present society is that this data harvesting often goes unnoticed and borderline without our full consent. We may agree to lengthy terms and conditions, but rarely do people truly understand what they’re putting out into the world by simply browsing social media and clicking on a few links. They have reason to be frustrated when at points they’re feeling stalked when visiting a certain website starts a chain of related ads in their searches and social media pages. These stalker ads are harmless enough, but the question of what exactly is being tracked and to what extent comes up at one point or another. 
  
  One can combat these fancy behavior analysis algorithms using insider content that only people within one’s own social circle would understand, but that content is being tracked, nonetheless. Simply not using certain platforms known for their heavy tracking could be an option, but to not use a widely used platform alienates those afraid for their privacy and limits their use of the internet. We may be reaching a point where anything being done on the Internet is tracked by someone or another and where law must be stringently put in place to prevent it.

## Security & Privacy as a Social and Cultural Phenomenon 
  
 Security is often directly thought of as a tech specific concept. Where there is security, there must be some fancy tech. Privacy on the otherhand is a socially constructed concept. It's arrived to by societies mutual agreement, often agreed upon through common law, that a concept, process, or thought is to remain secret and only be available to those within the circle of inclusion. These secrets vary culturally and among different social groups. 
 
  A group of teenages for example, share secrets in their social groups with a silently agreement that it shouldn't be shared with outsiders. However, with technology secrets are harder to keep secret. So, it is then that security plays a role in allowing for these secrets to hold their integrity as they would have in the physical world. We must then consider that security is not simply a technical feature to impliment, but rather a feature that comes out of the necessity to faciliate the traditional social consideration better known as privacy.
  
## Trust vs Trustworthiness
  
  Trust is mistakingly used to describe trustworthiness and vice versa. The two differ in meaning as well as intent when attempting to instill either in the tech world. As the old saying goes, trust is something that is earned. This stands true on the Internet as well. In order for trust to be built in say, a particular website, there must be an exchange of reliance that over repeated success, forms the chains of trust we traditionally understand between people. To say I trust a website, means that the person has had a history of success in using that website. 
  
  Trustworthiness by contrast is the expectation that a website is going to be able to perform and allow for the building of trust to occur. This expectation can arise from assurances linked to the website in the form of other well known websites or people advocating positively for the site. These assurances often provide a quicker route to building trust, but as the article points out, that trust is broken easier if those assurances hadn't been there to help with process.
  
  To say I trust technology should be understood as trust in the developers, as saying piece of technology programmed to perform tasks is trustworthy doesn't align with the meaning of the word. Understanding how people build trust and have certain expectations of trustworthiness helps in the development of technology. The more people trust a technology, the more willing they will be to invest their time and money into it.

### References
Boyd, D. 2012. “Networked Privacy.” Surveillance and Society. Published 2012.
Cheshire, C. 2011. “Online Trust, Trustworthiness, or Assurance?” Daedalus, the Journal of the American Academy of Arts & Sciences, vol. 140 (4), 2011, pp. 49-58.
Dourish, P. & Anderson, K. 2006. “Collective Information Practice: Exploring Privacy and Security as Social and Cultural Phenomena” Human-Computer Interaction, vol. 21, 2006, pp. 319-342

# Week 6

## This World of Ours

  The author seems very cynical of the security needs of the world and accurately describes why he is. Asking for improved encryption that make for single sentence messages with 95% added encryption overhead is overkill. The needs for encryption has it's place. I got the sense that the author was almost advocating for just enough security to prevent people from getting hacked by most attempt, provided they don't click on that link, and that getting hacked from that experienced computer savant is unsecurable and attempting so would be futile. 
  
  The world isn't ready for people to adopt concepts of encryption in their everyday lives if they're the ones that have to implement it. Getting the next generation interested is difficult at best, and as such the approach on security should be with all that in mind. The majority of computer users aren't informed and many will click the link to win a new iPad. So, forces such harsh encryption improvements when people click the link anyways seems pointless.
  
## Why Johnny Can't Encrypt

  The paper follows a scientific case study performed on the usability of PGP 5.0 and its ease of use for novice computer users. The experiment chose PGP as it was the most accessible and user-friendly encryption service at the time. It serves to prove that users who may argue they are technologically savvy cannot even perform the basic recommended encryption for conversing online. What good is a service that no one knows how to use properly. 
  
  Recommending security on a system will fall on deaf ears if they have no idea how to implement it. That's the challenge in designing technology moving forward. There needs to be a box you stick text in and a send button, with encryption entirely in the background if we want there to be encryption all the time. Users cannot be expected to understand how technology works, because they have no reason to learn. A hack on their credit card may be incentive, but they're not going to take up a two-year course in cybersecurity afterwards.
  
### References
Mickens, James. “This World of Ours”. Published 2014.
Salma, Sasse, Bonneau, Danilova, Naiakshina, Smith. “Obstacles to the Adoption of Secure Communication Tools”.
Whitten & Tygar. “Why Johnny Can’t Encrypt: A Usability Evaluation of PGP 5.0”.
  
# Week 7

## Sorting Things Out

  The paper follows the history of racism in South Africa. A time where people were classified by color in not just a black versus white sense, but of government defined categories of race. These categories had White, Asian, Indian, Colored, Black and a myriad of classifications that fell in between. The system failed people who primarily fell in the in-between category. The issue wasn't necessarily even that the skin was decided on a whim to be too dark to fit the category, but absurd hair checks, social affiliation, genealogy, and more. 
  
  If the classifications were held in infinitum, then maybe there would have been less problems. But, of course the first classification could have been done poorly (as it appears was the case far too much) and people needed to fit in to the category they felt "comfortable" in. I can understand attempting to process so many requests bog the system down for months if not years, but people had their lives put on complete hold because they weren't allowed to work or send their children to school or risk "tampering" their case. It really is a disturbing reality that shows the dangers in even attempting to segregate the functions of humans by such an objective categorization. 
  
   While I understand, the social devastation in this example was extreme and easy enough to identify, it's difficult to relate to information systems in its entirety. The basic principle of pooling people by the data you have on them can be dangerous as you risk making assumptions. The categorization in information technology is not so detrimental to everyday functioning as the paper seems to imply. Some systems do record "age, location, and expertise", but that isn't in most cases being used to restrict the users from accessing information. If it is, that's an obvious violation of the freedom that the technology is supposed to allow.

## Body, Biometrics, and Identity
 
  There are people who will argue that submitting themselves to biometric databases infringes on their freedoms to own their humanity. A bold, but arguably misguided and naive statement to make. To refuse to adhere to policies in place by a country that require submitting facial and partial body (fingerprint) recognition is certainly one's right, but it is the right of that country to refuse entrance by doing so. Philosophers have made a case that it is dehumanizing to an individual to submit details of the human form for identification purposes as often the process involves an oversimplification of the human form. After-all, we are not just shapes of skin with eyes and fingerprints. We are all unique individuals who don't fit any one mold that these biometric scanners attempt to conform us into. These arguments appear to make wrong of a situation that has no negative outcome on them outside of mental infringements to intangible concepts of self-ownership. We all understand you want to own your bodies intricate details. The extra data that biometrics will inevitable obtain will not be used to steal your property or your rights as a human. The aim is to allow for a more efficient identification system that removes tireless hours of man-powered verification to allow that time to be spent on improving society in other ways. Establishing a biometric identification system is of course going to have its problems, with identity theft being the most likely case. To deny the progression of biometric technology is to deny the advancement of society. Why waste countless hours verifying identity through forms and chains of approval, if a system could provide a biometric grid of complete unique identification.

## Experimentation in humanitarian locations

  The paper follows a heavy ontological approach to the science and technology of biometrics and its effects on society. At times I felt the paper went in circles brandishing the term "consituted technology" after explaining the pros and cons of biometrics. In a nutshell, there are pros and cons to biometrics. This "constitutes" as the paper so often states, the biometric technology that when taken from a science and technology study approach is entirely positive. The issue arises when individuals make claim to their human rights. The claim should be mute as the cons of identity theft are understood. No one wants their identity stolen and find themselves with shot credit and debt collectors chasing them endlessly. The goal in the refugee case is assurance that people coming in are documented so they both function as seemingly more normal humans in society and be identified if they chose to break the law.
  
  To shun technology is one thing, but to be skeptical and approach it with caution is another. 

### References
Bowker, G. C., & Star, S. L. (2008). Sorting things out: classification and its consequences. Cambridge, MA: MIT Press.
Jacobsen, K. L. (2015). Experimentation in humanitarian locations: UNHCR and biometric registration of Afghan refugees. Security Dialogue, 46, 144–164.
Mordini, E. (2008). Body, Bio-metrics and Identity. Bioethics, 22(9), 488–498. doi: 10.111/j.1467-8519.2008.00700.x

# Week 8

Search engines have no doubt access to immense pools of data. When you search for something, your given results that are hopefully relevant to what you need. With so many possible sources of information, it's hard to not wonder why a particular search always gives certain sites. It's also naive to not consider that these huge search engines are free of bugs and cannot be manipulated. When you examine how they function, the point is to provide the most relevant information that the most people have clicked on. If someone clicks on something, there's a good chance that means the link is relevant and what they're looking for. With enough visits and clicks, these links can be manipulated to be higher up on the relevant charts.

Clearly, there is alot to gain from being able to force content to users without them knowing. This paves the way for possible mental manipulation as the people start to see slews of similar data reinforcing ideas. This type of manipulation can be political or purely financial, but regardless it's clear we need to create environments free of these bad actors attempting to pursuade populations. The difficulty is puting in systems that catch metadata manipulation in a way that doesn't collect data that could be infringing on privacy rights/laws.

# AI Case Study
<p align="center"> 
<b> <h2>AI & Machine Learning* <br>
Why artificial intelligence is closer than people think <br>
Leo Kravchenko </h2><br>

Cybersecurity, Western Washington University, Bellingham, WA, United States, leokravchenko@yahoo.com
  </b><br>
  </p>
## ABSTRACT
Artificial intelligence is humankinds means to a more advanced future. The human mind is painfully slow at learning and in order for major advancements in technology to occur, we need to develop all of our efforts into this technology. A physicist can spend twenty or so years learning how and why the world works before ever attempting to develop any new theories. Similarly, a person can devote the same amount of time on mastering a game like Go, before attempting to create game breaking new strategies. A properly coded self-learning AI system can potentially learn both in a matter of months if not less. It’s clear that we need to embrace technology in order to improve upon it, however the dangers to human kind are great and as such the level of trust needed is going to be well established before we can start pouring all our efforts into this revolutionizing field.  

## Introduction
The artificial intelligence field has been advancing faster than expected in the past decade and with it comes a growing need to consider the repercussions of such a powerful form of technology. It’s human nature to want to find the next big leap in development that is stronger, bigger, and better than before. If one looks at the nuclear bomb race, it’s clear to see that some people didn’t fully understand what potential dangers were involved in managing nuclear products. If we handle artificial intelligence like we do a potential nuclear reactor explosion, then maybe there is hope for the human race.
## Potential of Artificial Intelligence?
When the general population is asked what their take on AI is, they usually respond in a post-apocalyptic vision of the world with mankind fighting off omnipotent robots. Albeit the result of the entertainments industries spin on AI, this could very well be a reality in the nearby future. There are advancements in robotics that can mimic nearly every motion a human is able to perform, including pulling a trigger on a gun. Paired with a program that mimics human cognition, this robot could start to learn how to aim nearly perfect in just a few months or days with todays AI technology. Now, copy that program to however many more robots you need, and you have an army of death.
What’s even more scary is a robot that can learn to decide on its own, what targets are the enemy and are deemed a big enough threat to shoot. It may seem impossible to most people, but there are systems currently that are capable of what we understand to be learning. However, rather than attempt to further scare the general population, there are of course much more lifesaving applications of this technology.
There is robotics used in surgery today to perform incredibly precise incisions in high risk procedures, that allow for the removal of human error. All it takes is a small cut in the wrong place to cause a person to bleed out on the operating table. These systems rely on a surgeon to handle the two robotic arms, but it’s not too far off in the future where AI could be introduced to perform the surgery without any outside assistance. 
## What is AI? 
Very quickly computer scientists discovered that if AI is to come into fruition, that attempting to program every possible scenario within a given task leads to a dead end in available computing resources. The first demonstratable use of AI was back in 1951, used to master the tasks involved in checkers. Since then the system has improved on and mapped out every possible game board setup that could follow after a player moves a single piece. It’s hard to fully grasp, but there exists 10 to the 31 possible setups. 
However, this system cannot be expanded on to any game more complicated such as chess as the number of game boards increases to 10 to the 123. The sheer computational power needed to map and traverse every possible game in the tree is too great for modern technology. So, computer scientists needed to find a way to mimic learning the game through trial and error. Much like the human learning process, which relies on repeated exposure to a single given task, machines were programmed to follow the same process of learning. 
The design of trial and error is simple in principle but complicated in the coding necessary to achieve it. With chess, the system had to be given time to “see” or store matches played by master level chess players. Once the games were stored, they were analyzed play by play and given probabilities of success rates. In other words, moving a piece to a specific place on the board given the current board layout has a certain probability of resulting in a win. Of course, this type of probability reasoning requires mass amounts of known moves in order to be successful. 
The beauty of computers is their ability to perform simple tasks near instantaneously. So, one can imagine how quick this chess system can play through a single game against itself. Simply let the machine play against itself for millions of iterations, and you will get a near unbeatable system. This principle has since been successfully applied to the game of Go, which has 10 to the 360 game boards in its tree, as well as more intricate games like poker. What used to be considered hundreds of years away from being developed has been conquered.
## How does conquering games translate?
While impressive, creating systems that are unbeatable in games of strategy don’t mean much for society. However, the potential of applying the same principles of “machine learning” can be applied to virtually any task. A similar approach was taken to teach a four limbed robot how to walk. Not only did this robot learn how to walk, albeit in a unconventional and unexpected way, but it learned to do so without any information about it’s limbs, how to use them, and examples of how to use them. In other words, this robot learned what its own body looked like, what actions created motions, and ultimately what actions needed to be done on its four limbs to move across the floor. 
The same artificial intelligence algorithms that can be used to teach walking can be taught to perform any other human function. All forms of manual labor could eventually be entirely replaced by robots, leaving just a few jobs for maintenance and repair. Factory work, construction, surgeries, dental work, and any other hands on work has the potential to be performed by these systems. Hopefully with the improvements in computational power such as quantum computing, these algorithms can learn and perform at a rate that surpasses the human mind.
## Trust Issues within AI Technology
There is undoubtedly major distrust in the computer world. With computers being broken into regularly and major privacy breaches happening to once trusted companies, it will be an uphill battle implementing AI robotics into the medical field. There exists an immediate trust when a new technology that becomes available, that is more fragile than a trust relationship built over time between humans. If a surgeon has performed ten dangerous surgeries with no complications, they have started to build this foundation of trust that allows people to be at ease that they’ll be fine going into surgery without ever speaking to the surgeon. Now if there was a robot that performed the same ten surgeries without complications, the same person may or may not trust the robot to perform their surgery. If robotic surgery was the norm, then of course the trust would be there, but it’s getting to that point where we embrace the technology and allow for it to become the norm that is difficult.
When designing regular software, with simple tasks, it’s far too common to release systems with bugs and vulnerabilities within the system. The idea that the software functions well enough, with almost no issues is a notion shared by the tech industry largely due to the need of funding. Development costs money and depending on how complicated a system becomes, can run out funds faster than expected. So, we find these swiss cheese programs flocking into the public that quickly become abused by the inevitable slew of hackers attempting to make some money off other’s inability to secure a system.
This cannot be the method in which AI is released onto public affecting systems. There needs to be a level of security that is impenetrable with a failsafe in place to shut down the system. The worst future imaginable would be if one group of people were to create an all-powerful AI that could rule the world. As Elon Musk, a leading expert in modern AI, said “for an AI, there would be no death, we would have an immortal dictator.” The key to developing these systems is to democratize them and ensure the impacts and dangers are considered and addressed before continuing. 
## What are the dangers in AI?
We have already discussed killer robots and omnipotent AI dictators. It may not be clear how we would arrive at either point. Automation is the key to the advancement of the human race. To be able to let a computer figure out a task that could take humans hundreds of years to draw out a conclusion or see a pattern is the obvious choice. The danger is this technology is no longer just limited to outside abuse. We no longer have to just consider a hacker or insider gaining access and wreaking havoc on the world. Now, the system itself is a threat. The algorithms used in the beginning stages a system’s “machine learning” are obviously understood by the programmers. However, when the system has gone through millions of iterations of self-learning, it reacts in ways that the programmers do not understand. It’s in this loss of understanding that the danger lies. A machine will perform what it has “learned” to do. If a machine somehow concludes that the easiest way to win a game of Go is to simply kill the opposing player, then there is nothing in the system’s database from stopping it from doing so. There are no ethical codes or emotion involved. 
For AI to continue and be implemented in various forms, there needs to be a deep understanding of where the system’s knowledge is headed and how it arrived at that conclusion. 
